# TokenizaciÃ³n (Tokenization)

- Un **token** no siempre es una palabra completa.
    
    Puede ser:
    
    - una palabra entera (ej. *dog*),
    - parte de una palabra (ej. *com* + *puter*),
    - o incluso un sÃ­mbolo (*?*, *#*, etc.).

Esto depende de cÃ³mo se haga la **tokenizaciÃ³n**. Por eso los LLM no trabajan directamente con â€œpalabrasâ€, sino con estas unidades mÃ­nimas llamadas tokens.

# **What is tokenization?**

Tokenization is the first step and the last step of text processing and modeling. Texts need to be represented as numbers in our models so that our model can understand. Tokenization breaks down text into tokens, and each token is assigned a numerical representation, or index, which can be used to feed into a model. In a typical LLM workflow:

- We first encode the input text into tokens using a tokenizer. Each unique token is assigned a specific index number in the tokenizerâ€™s vocabulary.
- Once the text is tokenized, these tokens are passed through the model, which typically includes an embedding layer and transformer blocks. The embedding layer converts the tokens into dense vectors that capture semantic meanings. Check out ourÂ [embedding guide](https://docs.mistral.ai/capabilities/embeddings/overview/)Â for details. The transformer blocks then process these embedding vectors to understand the context and generate results.
- The last step is decoding, which detokenize output tokens back to human-readable text. This is done by mapping the tokens back to their corresponding words using the tokenizerâ€™s vocabulary.

Tokenization is the first step in transforming raw text into a format that machine learning models can understand. Today, weâ€™ll explore how to implement tokenizers in Python using the Hugging Face library.

ğŸ“– What youâ€™ll learn today:

ğŸ”¹ Loading Pre-trained Tokenizers â€“ Leverage (aprovechar) powerful models like bert-base-uncased.

ğŸ”¹ Tokenizing Text â€“ Converting raw text into token IDs, attention masks, and more.

ğŸ”¹ Passing Tokens to Models â€“ Using tokenized inputs to extract meaningful embeddings.

ğŸ”¹ Model Outputs â€“ Understanding last_hidden_state and pooler_output.

By the end of today's lesson, you'll be able to implement tokenizers effortlessly and prepare your text data for advanced NLP tasks.

![image.png](assets/images/image%2018.png)

To prepare language inputs for transformers, we convert an input sequence into tokens and then into input embeddings. At a high level, an input embedding is a high-dimensional vector that represents the meaning of each token in the sentence. This embedding is then fed into the transformer for processing

![image.png](assets/images/image%2019.png)
